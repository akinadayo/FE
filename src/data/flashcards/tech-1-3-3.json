{
  "topicId": "tech-1-3-3",
  "title": "符号理論",
  "flashcards": [
    {
      "id": "fc-tech-1-3-3-001",
      "front": "情報量とは何ですか？",
      "back": "ある事象が起こった時の「驚き」や「意外性」を数値化したものです。\n\n計算式:\nI = log₂(1/p) = -log₂(p)\n\np: 事象の確率\n\n確率が低い事象ほど情報量が大きい。",
      "importance": 5
    },
    {
      "id": "fc-tech-1-3-3-002",
      "front": "確率1/2の事象の情報量は？",
      "back": "1ビットです。\n\n計算:\nI = log₂(1/0.5)\n  = log₂(2)\n  = 1 ビット\n\n例: コイン投げで表が出る（確率1/2）→1ビット",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-003",
      "front": "確率1/8の事象の情報量は？",
      "back": "3ビットです。\n\n計算:\nI = log₂(1/0.125)\n  = log₂(8)\n  = 3 ビット\n\n例: サイコロを3回振って全部1が出る",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-004",
      "front": "エントロピーとは何ですか？",
      "back": "情報源が発生する情報の平均情報量です。不確実性の度合いを表します。\n\n計算式:\nH = -Σ p(x) log₂ p(x)\n\np(x): 各事象の確率\n\n均等分布で最大、偏りがあると小さくなる。",
      "importance": 5
    },
    {
      "id": "fc-tech-1-3-3-005",
      "front": "2値（0と1が等確率）のエントロピーは？",
      "back": "1ビットです。\n\n計算:\nH = -0.5×log₂(0.5) - 0.5×log₂(0.5)\n  = -0.5×(-1) - 0.5×(-1)\n  = 0.5 + 0.5\n  = 1 ビット\n\nこれが最大エントロピー。",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-006",
      "front": "ハフマン符号化とは何ですか？",
      "back": "出現頻度の高い文字に短い符号を割り当てる可逆圧縮法です。\n\n手順:\n1. 文字を出現頻度順に並べる\n2. 最小の2つを結合\n3. 木構造を作成\n4. 各枝に0/1を割り当て\n\n平均符号長が最小になる。",
      "importance": 5
    },
    {
      "id": "fc-tech-1-3-3-007",
      "front": "ハフマン符号化の利点は？",
      "back": "可逆圧縮で最適な平均符号長を実現できることです。\n\n特徴:\n・元データを完全に復元可能\n・出現頻度に応じた可変長符号\n・平均符号長がエントロピーに近い\n\n欠点: 頻度表が必要",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-008",
      "front": "ランレングス符号化とは何ですか？",
      "back": "同じデータの連続を「データ×個数」で表現する圧縮法です。\n\n例:\nAAAAABBBCCC\n↓\nA×5, B×3, C×3\n\n単純な画像やFAXで効果的。",
      "importance": 5
    },
    {
      "id": "fc-tech-1-3-3-009",
      "front": "ランレングス符号化が効果的な場合は？",
      "back": "同じデータが連続して出現する場合です。\n\n効果的:\n・白黒画像\n・FAXデータ\n・単色の多い図形\n\n非効果的:\n・写真（色が頻繁に変わる）\n・ランダムなデータ",
      "importance": 3
    },
    {
      "id": "fc-tech-1-3-3-010",
      "front": "符号化効率とは何ですか？",
      "back": "エントロピーと平均符号長の比です。\n\n計算式:\n効率 = H（エントロピー）/ L（平均符号長）\n\n1に近いほど効率が良い圧縮。\nハフマン符号化は効率が1に近い。",
      "importance": 3
    },
    {
      "id": "fc-tech-1-3-3-011",
      "front": "平均符号長とは何ですか？",
      "back": "各文字の符号長を出現確率で重み付けした平均値です。\n\n計算式:\nL = Σ p(x) × l(x)\n\np(x): 文字xの出現確率\nl(x): 文字xの符号長\n\n圧縮効果の指標。",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-012",
      "front": "一意復号可能性とは？",
      "back": "符号化されたデータを元のデータに一意に復号できる性質です。\n\n条件:\n・どの符号も他の符号の接頭語にならない（prefix条件）\n\n例:\nOK: A=0, B=10, C=110\nNG: A=0, B=01（Aが0, Bの接頭語が0）",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-013",
      "front": "固定長符号と可変長符号の違いは？",
      "back": "固定長: すべての文字が同じビット数\n可変長: 文字ごとに異なるビット数\n\n固定長: ASCII（7ビット）\n可変長: ハフマン符号、UTF-8\n\n可変長の方が圧縮効率が高い。",
      "importance": 4
    },
    {
      "id": "fc-tech-1-3-3-014",
      "front": "シャノンの符号化定理とは？",
      "back": "データを圧縮できる限界はエントロピーであるという定理です。\n\n意味:\n・平均符号長 ≥ エントロピー\n・エントロピー以下には圧縮不可\n・ハフマン符号化はこの限界に近い\n\n情報理論の基本定理。",
      "importance": 3
    }
  ]
}
