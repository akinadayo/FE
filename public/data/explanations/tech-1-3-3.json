{
  "topicId": "tech-1-3-3",
  "title": "符号化",
  "pages": [
    {
      "id": 1,
      "title": "情報量の基礎",
      "sections": [
        {
          "type": "text",
          "content": "情報理論では、情報の量を数値で表します。情報量は「不確実性の減少量」として定義され、ビット単位で測定されます。"
        },
        {
          "type": "highlight",
          "icon": "📝",
          "title": "情報量の定義",
          "content": "ある事象が確率pで起こるとき、その事象の情報量Iは：\n\nI = log₂(1/p) = -log₂(p)  [ビット]\n\n確率が低い事象ほど、起きたときの情報量は大きくなります。"
        },
        {
          "type": "example",
          "title": "情報量の計算例",
          "content": "コインを投げて表が出る確率：p = 1/2\n\n情報量 I = log₂(1/0.5)\n         = log₂(2)\n         = 1ビット\n\nサイコロで1が出る確率：p = 1/6\n\n情報量 I = log₂(1/0.167)\n         ≈ log₂(6)\n         ≈ 2.58ビット\n\n→ 確率が低いサイコロの方が情報量が大きい"
        },
        {
          "type": "highlight",
          "icon": "💡",
          "title": "情報量の性質",
          "content": "・確率が低い事象ほど情報量が大きい\n・確率1（必ず起こる）の事象の情報量は0\n・確率0.5の事象の情報量は1ビット\n・独立な2つの事象の情報量は足し算できる"
        },
        {
          "type": "table",
          "title": "確率と情報量の関係",
          "headers": ["確率", "情報量"],
          "rows": [
            ["1（必ず起こる）", "0ビット"],
            ["1/2", "1ビット"],
            ["1/4", "2ビット"],
            ["1/8", "3ビット"],
            ["1/16", "4ビット"]
          ]
        }
      ]
    },
    {
      "id": 2,
      "title": "エントロピー（平均情報量）",
      "sections": [
        {
          "type": "text",
          "content": "エントロピーは、情報源全体の平均的な情報量を表す指標です。データ圧縮の理論的限界を示します。"
        },
        {
          "type": "highlight",
          "icon": "📝",
          "title": "エントロピーの定義",
          "content": "確率分布がp₁, p₂, ..., pₙの情報源のエントロピーHは：\n\nH = Σ pᵢ × log₂(1/pᵢ)\n  = -Σ pᵢ × log₂(pᵢ)  [ビット/シンボル]\n\nエントロピーが小さいほど、データは圧縮しやすい。"
        },
        {
          "type": "example",
          "title": "エントロピーの計算例",
          "content": "4種類の文字A, B, C, Dが以下の確率で出現する：\n\nA: 1/2\nB: 1/4\nC: 1/8\nD: 1/8\n\nエントロピーH：\nH = -(1/2)×log₂(1/2) - (1/4)×log₂(1/4)\n    - (1/8)×log₂(1/8) - (1/8)×log₂(1/8)\n  = (1/2)×1 + (1/4)×2 + (1/8)×3 + (1/8)×3\n  = 0.5 + 0.5 + 0.375 + 0.375\n  = 1.75ビット/文字\n\n→ 1文字あたり平均1.75ビットで表現できる"
        },
        {
          "type": "highlight",
          "icon": "💡",
          "title": "エントロピーの性質",
          "content": "・すべての記号が等確率のとき、エントロピーは最大\n・偏りが大きいほど、エントロピーは小さい\n・エントロピーが圧縮の理論的限界を示す\n\n例：n種類の等確率シンボル\nH = log₂(n) ビット/シンボル"
        },
        {
          "type": "example",
          "title": "等確率の場合",
          "content": "8種類の文字が等確率（各1/8）で出現：\n\nH = log₂(8) = 3ビット/文字\n\n→ 最低でも1文字あたり3ビット必要\n→ これより小さくは圧縮できない（理論的限界）"
        }
      ]
    },
    {
      "id": 3,
      "title": "ハフマン符号化",
      "sections": [
        {
          "type": "text",
          "content": "ハフマン符号化は、出現頻度に応じて文字に異なる長さのコードを割り当てる可逆圧縮方式です。頻出する文字に短いコードを割り当てることで、全体のデータ量を削減します。"
        },
        {
          "type": "highlight",
          "icon": "📝",
          "title": "ハフマン符号化の特徴",
          "content": "・可逆圧縮（元のデータを完全に復元可能）\n・出現頻度が高い文字に短いコードを割り当てる\n・プレフィックスコード（接頭符号）になる\n・エントロピーに近い圧縮率を実現"
        },
        {
          "type": "text",
          "content": "ハフマン符号化の手順："
        },
        {
          "type": "list",
          "items": [
            "各文字の出現頻度を数える",
            "出現頻度の小さい2つをまとめて木を作る",
            "これを繰り返してハフマン木を構築する",
            "根からの経路で符号を決定（左:0、右:1）"
          ]
        },
        {
          "type": "example",
          "title": "ハフマン符号化の例",
          "content": "文字列「AAAAABBBCCD」を符号化：\n\n【出現頻度】\nA: 5回\nB: 3回\nC: 2回\nD: 1回\n\n【ハフマン木の構築】\n1. D(1)とC(2)を結合 → DC(3)\n2. B(3)とDC(3)を結合 → BDC(6)\n3. A(5)とBDC(6)を結合 → 完成(11)\n\n【符号の割り当て】\nA: 0      （1ビット、最頻出）\nB: 10     （2ビット）\nC: 110    （3ビット）\nD: 111    （3ビット、最低頻度）\n\n【圧縮結果】\n元のデータ：11文字\n固定長（2ビット）：22ビット\nハフマン符号化：5×1 + 3×2 + 2×3 + 1×3 = 20ビット\n\n圧縮率：20/22 ≈ 91%"
        },
        {
          "type": "highlight",
          "icon": "📝",
          "title": "プレフィックスコード",
          "content": "ハフマン符号は「プレフィックスコード（接頭符号）」です。\n\nどの符号も他の符号の先頭部分にならないため、区切り記号なしで一意に復号できます。\n\n例：\nA: 0\nB: 10\nC: 110\nD: 111\n\n符号列「01011110」は「A B A D C」と一意に復号可能"
        },
        {
          "type": "table",
          "title": "符号化方式の比較",
          "headers": ["方式", "文字Aのコード", "平均ビット長"],
          "rows": [
            ["固定長（2ビット）", "00", "2.0ビット/文字"],
            ["ハフマン符号", "0", "1.82ビット/文字"]
          ]
        }
      ]
    },
    {
      "id": 4,
      "title": "ランレングス符号化",
      "sections": [
        {
          "type": "text",
          "content": "ランレングス符号化は、同じデータが連続する場合に、「データの値」と「連続回数」のペアで表現する圧縮方式です。"
        },
        {
          "type": "highlight",
          "icon": "📝",
          "title": "ランレングス符号化とは",
          "content": "連続する同じデータを「値×回数」の形で表現します。\n\n例：\nAAAABBBBBCCCC → A4B5C4\n000000111110000 → 0×6, 1×5, 0×4\n\n単純で実装が容易な可逆圧縮方式です。"
        },
        {
          "type": "example",
          "title": "ランレングス符号化の例（画像）",
          "content": "白黒画像の1行のデータ：\n\n元データ（0:白、1:黒）：\n0000000001111100000000111111110000\n\n符号化後：\n(0,8)(1,5)(0,10)(1,8)(0,4)\n\nまたは：\n8,5,10,8,4（先頭が0で開始と決めれば値は不要）\n\n白や黒が連続する画像（FAX、ロゴなど）で効果的"
        },
        {
          "type": "highlight",
          "icon": "💡",
          "title": "ランレングス符号化の特徴",
          "content": "利点：\n・アルゴリズムが単純\n・高速に圧縮・展開可能\n・連続するデータで高い圧縮率\n\n欠点：\n・連続しないデータでは逆に増える\n・一般的なテキストには不向き\n\n適した用途：\n・FAX画像\n・単純な図形\n・動画のフレーム間圧縮"
        },
        {
          "type": "example",
          "title": "圧縮効率の比較",
          "content": "【効率が良い例】\nAAAAAAAAAA（10文字）\n→ A10（2バイト程度）\n圧縮率：20%\n\n【効率が悪い例】\nABCDEFGHIJ（10文字）\n→ A1B1C1D1E1F1G1H1I1J1（20バイト程度）\n圧縮率：200%（逆に増加！）\n\n→ 連続性のないデータには不向き"
        },
        {
          "type": "table",
          "title": "用途別の符号化方式",
          "headers": ["データの特徴", "適した符号化", "例"],
          "rows": [
            ["連続性が高い", "ランレングス", "FAX、ロゴ画像"],
            ["出現頻度に偏り", "ハフマン", "テキスト、HTML"],
            ["一般的なデータ", "LZ系圧縮", "ZIP、gzip"],
            ["画像", "JPEG", "写真"],
            ["音声", "MP3", "音楽"]
          ]
        }
      ]
    },
    {
      "id": 5,
      "title": "符号化の応用と実践",
      "sections": [
        {
          "type": "text",
          "content": "符号化技術は、データ圧縮だけでなく、誤り訂正や暗号化など、様々な分野で応用されています。"
        },
        {
          "type": "highlight",
          "icon": "📝",
          "title": "実用的な圧縮形式",
          "content": "多くの実用圧縮形式は、複数の符号化技術を組み合わせています。\n\nZIP（LZ77 + ハフマン符号化）\n・辞書式圧縮で重複パターンを検出\n・ハフマン符号化でさらに圧縮\n\nGIF（LZW + ランレングス）\n・辞書式圧縮\n・画像の連続性を利用"
        },
        {
          "type": "example",
          "title": "圧縮率の計算問題",
          "content": "問題：\n以下の文字列をハフマン符号化した場合の\n圧縮率を求めよ。\n\n文字列：AAAAAABBBCCD（11文字）\n\n【解答】\n出現頻度：A:6, B:3, C:2, D:1\n\nハフマン符号：\nA: 0 (1ビット)\nB: 10 (2ビット)\nC: 110 (3ビット)\nD: 111 (3ビット)\n\n圧縮後のビット数：\n6×1 + 3×2 + 2×3 + 1×3 = 21ビット\n\n固定長の場合（2ビット/文字）：\n11×2 = 22ビット\n\n圧縮率 = 21/22 ≈ 95.5%\n削減率 = (22-21)/22 ≈ 4.5%"
        },
        {
          "type": "highlight",
          "icon": "✅",
          "title": "試験対策のポイント",
          "content": "1. 情報量の計算式を覚える（I = log₂(1/p)）\n2. エントロピーの意味を理解する（圧縮の理論的限界）\n3. ハフマン符号化の手順を理解する\n4. ランレングス符号化の仕組みを理解する\n5. 各符号化方式の適用場面を区別できるようにする\n6. プレフィックスコードの性質を理解する\n7. 圧縮率の計算ができるようにする"
        },
        {
          "type": "table",
          "title": "符号化方式のまとめ",
          "headers": ["方式", "種類", "特徴", "用途"],
          "rows": [
            ["ハフマン", "可逆", "頻度に応じた符号長", "テキスト圧縮"],
            ["ランレングス", "可逆", "連続データを回数で表現", "画像、FAX"],
            ["LZ系", "可逆", "辞書式圧縮", "ZIP、gzip"],
            ["JPEG", "非可逆", "DCT変換", "写真"],
            ["MP3", "非可逆", "聴覚特性利用", "音楽"]
          ]
        },
        {
          "type": "highlight",
          "icon": "💡",
          "title": "覚えておきたい公式",
          "content": "情報量：I = log₂(1/p) ビット\n\nエントロピー：H = -Σ pᵢ log₂(pᵢ) ビット/シンボル\n\n等確率のエントロピー：H = log₂(n) ビット/シンボル\n\n圧縮率：圧縮後サイズ / 圧縮前サイズ × 100%"
        }
      ]
    }
  ]
}
