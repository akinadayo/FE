{
  "topicId": "tech-1-3-3",
  "title": "符号化",
  "questions": [
    {
      "id": "q-tech-1-3-3-001",
      "type": "multiple_choice",
      "question": "情報量の単位として正しいものはどれですか？",
      "options": ["バイト", "ビット", "ヘルツ", "デシベル"],
      "correctAnswer": 1,
      "explanation": "情報量の単位は「ビット（bit）」です。\n\n情報量の定義：\n・不確実性の大きさを数値化したもの\n・シャノンの情報理論に基づく\n・確率が低い事象ほど情報量が大きい\n\n情報量の計算式：\nI(x) = -log₂ P(x) [ビット]\n\nここで、P(x)は事象xが起こる確率\n\n例：\n・コインの表が出る（確率1/2）\n  I = -log₂(1/2) = 1ビット\n\n・サイコロで6の目が出る（確率1/6）\n  I = -log₂(1/6) ≈ 2.58ビット\n\n・確実に起こる事象（確率1）\n  I = -log₂(1) = 0ビット（情報なし）\n\nバイトはデータの大きさの単位、ヘルツは周波数の単位、デシベルは音の強さの単位です。"
    },
    {
      "id": "q-tech-1-3-3-002",
      "type": "multiple_choice",
      "question": "エントロピー（平均情報量）が最大になるのはどのような場合ですか？",
      "options": [
        "1つの事象の確率が1",
        "全ての事象が等確率",
        "1つの事象の確率が0",
        "事象が2つだけ"
      ],
      "correctAnswer": 1,
      "explanation": "エントロピーは、全ての事象が等確率のときに最大になります。\n\nエントロピー（平均情報量）の定義：\nH = -Σ P(xi) log₂ P(xi) [ビット]\n\n例1：コインの表裏（等確率）\nP(表) = 1/2, P(裏) = 1/2\nH = -(1/2)log₂(1/2) - (1/2)log₂(1/2)\n  = 1/2 + 1/2 = 1ビット（最大）\n\n例2：偏ったコイン\nP(表) = 0.9, P(裏) = 0.1\nH ≈ 0.47ビット（小さい）\n\n例3：確実な事象\nP(表) = 1, P(裏) = 0\nH = 0ビット（最小、不確実性がない）\n\n意味：\n・エントロピーが大きい：予測困難、ランダム性が高い\n・エントロピーが小さい：予測しやすい、規則性が高い\n\nデータ圧縮では、エントロピーが理論的な圧縮限界を示します。"
    },
    {
      "id": "q-tech-1-3-3-003",
      "type": "multiple_choice",
      "question": "ハフマン符号化の特徴として正しいものはどれですか？",
      "options": [
        "全ての文字を同じ長さで符号化",
        "出現頻度の高い文字を短い符号で表現",
        "データが必ず1/2に圧縮される",
        "非可逆圧縮である"
      ],
      "correctAnswer": 1,
      "explanation": "ハフマン符号化は、出現頻度の高い文字を短い符号で表現する可逆圧縮アルゴリズムです。\n\nハフマン符号化の原理：\n・出現頻度の高い文字：短いビット列\n・出現頻度の低い文字：長いビット列\n・平均符号長を最小化\n\n例：\n文字列「AAABBCD」\n\n出現回数：\nA: 3回, B: 2回, C: 1回, D: 1回\n\nハフマン木の構築：\n1. 出現頻度の低い文字から組み合わせる\n2. 木を作成し、左枝を0、右枝を1とする\n\n符号化の例：\nA: 0 (1ビット)\nB: 10 (2ビット)\nC: 110 (3ビット)\nD: 111 (3ビット)\n\n元のデータ（固定長3ビット）：7文字 × 3ビット = 21ビット\nハフマン符号化：3×1 + 2×2 + 1×3 + 1×3 = 13ビット\n圧縮率：13/21 ≈ 62%\n\n特徴：\n・可逆圧縮（完全に復元可能）\n・接頭符号（prefix-free code）：他の符号の接頭辞にならない\n・理論的に最適な符号長（エントロピーに近い）\n\n用途：ZIP、PNG、JPEG、MP3などで使用されています。"
    },
    {
      "id": "q-tech-1-3-3-004",
      "type": "multiple_choice",
      "question": "ランレングス符号化で「AAABBBBBCC」を圧縮すると、どのように表現されますか？",
      "options": ["A3B5C2", "3A5B2C", "ABABABAB", "11文字"],
      "correctAnswer": 0,
      "explanation": "ランレングス符号化（Run Length Encoding）は、連続する同じ文字を「文字+繰り返し回数」で表現する圧縮方式です。\n\n元のデータ：「AAABBBBBCC」（11文字）\n\n圧縮後：「A3B5C2」\n・A：3回連続\n・B：5回連続\n・C：2回連続\n\nまたは「3A5B2C」の表記も可能（回数+文字）\n\nランレングス符号化の特徴：\n\n【適している場合】\n・同じデータが連続する（FAX、単純な画像など）\n・例：「0000000011111111」→「087181」\n\n【適さない場合】\n・連続しないデータ\n・例：「ABCDEF」→「A1B1C1D1E1F1」（逆に増加）\n\n【用途】\n・FAXの画像データ（白黒の連続）\n・BMP画像（一部）\n・簡単なアニメーション圧縮\n\n【利点】\n・シンプルで実装が容易\n・高速な圧縮・展開\n・可逆圧縮\n\n【欠点】\n・連続性がないと圧縮効果なし\n・場合によってはデータ量が増加\n\n実際の圧縮では、他の方式（ハフマン符号化、LZ77など）と組み合わせて使用されることが多いです。"
    },
    {
      "id": "q-tech-1-3-3-005",
      "type": "multiple_choice",
      "question": "次のデータのうち、ランレングス符号化で最も高い圧縮率が期待できるのはどれですか？",
      "options": [
        "「ABCDEFGH」",
        "「AAAAABBBBB」",
        "「ABABABABAB」",
        "「ABCABCABC」"
      ],
      "correctAnswer": 1,
      "explanation": "ランレングス符号化は、同じ文字が連続するデータで高い圧縮率を実現します。\n\n各データの圧縮結果：\n\n1. 「ABCDEFGH」（8文字）\n   →「A1B1C1D1E1F1G1H1」（16文字）\n   圧縮率：200%（逆に増加）\n\n2. 「AAAAABBBBB」（10文字）\n   →「A5B5」（4文字）\n   圧縮率：40%（最も効果的）\n\n3. 「ABABABABAB」（10文字）\n   →「A1B1A1B1A1B1A1B1A1B1」（20文字）\n   圧縮率：200%（逆に増加）\n\n4. 「ABCABCABC」（9文字）\n   →「A1B1C1A1B1C1A1B1C1」（18文字）\n   圧縮率：200%（逆に増加）\n\n結論：「AAAAABBBBB」が最も高い圧縮率\n\nランレングス符号化が有効なケース：\n・FAXの白黒画像（同じ色が連続）\n・単純なグラフィック\n・一定の背景色が続く画像\n\nランレングス符号化が無効なケース：\n・写真（色が複雑に変化）\n・テキストデータ（文字が頻繁に変わる）\n・ランダムなデータ\n\n実用的には、データの特性に応じて適切な圧縮アルゴリズムを選択する必要があります。"
    },
    {
      "id": "q-tech-1-3-3-006",
      "type": "multiple_choice",
      "question": "符号化理論において、「符号の効率」を高めるために重要なことはどれですか？",
      "options": [
        "全ての符号を同じ長さにする",
        "出現頻度に応じて符号長を変える",
        "符号を長くする",
        "ランダムに符号を割り当てる"
      ],
      "correctAnswer": 1,
      "explanation": "符号の効率を高めるには、出現頻度に応じて符号長を変えることが重要です。\n\nシャノンの符号化定理：\n・平均符号長の下限はエントロピー\n・最適な符号化では、出現頻度の高い記号に短い符号を割り当てる\n\n例：英文のアルファベット\n\n固定長符号（非効率）：\n・全ての文字を5ビットで表現（A～Zで26文字）\n・平均符号長：5ビット\n\n可変長符号（効率的）：\n・E（出現頻度12.7%）：短い符号（例：010）\n・Z（出現頻度0.07%）：長い符号（例：01110101）\n・平均符号長：約4.2ビット（ハフマン符号化の場合）\n\n符号の効率：\nη = H / L\n\nここで、\nH：エントロピー（平均情報量）\nL：平均符号長\n\n理想的な符号化ではη = 1（100%）に近づきます。\n\nハフマン符号化やシャノン符号化は、この原理に基づいて最適な符号を生成します。\n\n固定長符号の利点：\n・処理が簡単\n・ランダムアクセス可能\n\n可変長符号の利点：\n・平均符号長が短い\n・圧縮率が高い\n\nデータの特性に応じて適切な符号化方式を選択することが重要です。"
    },
    {
      "id": "q-tech-1-3-3-007",
      "type": "multiple_choice",
      "question": "情報源のエントロピーが1ビットの場合、最適な符号化での平均符号長の理論的下限はどれですか？",
      "options": ["0.5ビット", "1ビット", "2ビット", "下限はない"],
      "correctAnswer": 1,
      "explanation": "シャノンの符号化定理により、平均符号長の理論的下限はエントロピーに等しくなります。\n\nシャノンの符号化定理：\nH ≤ L < H + 1\n\nここで、\nH：エントロピー（平均情報量）\nL：平均符号長\n\n理論的下限：L ≥ H\n\nエントロピーが1ビットの場合：\n・理論的下限：1ビット\n・これ以上は圧縮できない\n\n例：等確率の2値情報源\nP(0) = 0.5, P(1) = 0.5\nH = -0.5 log₂(0.5) - 0.5 log₂(0.5) = 1ビット\n\n最適符号化：\n0 → 0（1ビット）\n1 → 1（1ビット）\n平均符号長：1ビット = エントロピー\n\nエントロピーの意味：\n・データの持つ本質的な情報量\n・圧縮の理論的限界\n・ランダム性の指標\n\n実際の圧縮：\n・可逆圧縮：エントロピーが下限\n・非可逆圧縮：エントロピー以下も可能（情報を削除）\n\n例：\n・JPEG：視覚的に重要でない情報を削除\n・MP3：聴覚的に重要でない情報を削除\n\nエントロピーは、データの「圧縮できなさ」を表す指標とも言えます。"
    },
    {
      "id": "q-tech-1-3-3-008",
      "type": "multiple_choice",
      "question": "ハフマン符号の性質として正しいものはどれですか？",
      "options": [
        "他の符号の接頭辞になることがある",
        "全ての符号が一意に復号できる",
        "非可逆圧縮である",
        "固定長符号である"
      ],
      "correctAnswer": 1,
      "explanation": "ハフマン符号は、全ての符号が一意に復号できる性質（一意復号可能性）を持ちます。\n\nハフマン符号の重要な性質：\n\n1. 接頭符号（Prefix-free code）\n   ・どの符号も他の符号の接頭辞にならない\n   ・例：A=0, B=10, C=110, D=111\n   ・0は10、110、111の接頭辞ではない\n\n2. 一意復号可能性\n   ・符号化されたビット列を一意に復号できる\n   ・区切り記号が不要\n\n例：\n符号化：A=0, B=10, C=110\nビット列「0100110」の復号：\n・0（A）\n・10（B）\n・0（A）\n・110（C）\n→ 「ABAC」と一意に復号\n\n反例（悪い符号）：\nA=0, B=01, C=011\n・0は01と011の接頭辞\n・ビット列「011」は「A+11」か「B+1」か「C」か判別不能\n\nハフマン符号の構築：\n1. 出現頻度の低い記号から組み合わせる\n2. 二分木を構成\n3. 左枝を0、右枝を1として符号を割り当てる\n\nこの方法により、自動的に接頭符号が生成され、一意復号可能性が保証されます。\n\nその他の性質：\n・可変長符号（出現頻度により長さが異なる）\n・可逆圧縮（完全に復元可能）\n・平均符号長が最小（最適性）"
    },
    {
      "id": "q-tech-1-3-3-009",
      "type": "multiple_choice",
      "question": "次の文字列のうち、ハフマン符号化で最も圧縮効果が高いのはどれですか？",
      "options": [
        "「ABCDEFGH」（全て1回ずつ）",
        "「AAAABBCD」（Aが多い）",
        "「AABBCCDD」（均等）",
        "「ABCDABCD」（繰り返し）"
      ],
      "correctAnswer": 1,
      "explanation": "ハフマン符号化は、出現頻度に偏りがあるほど圧縮効果が高くなります。\n\n各文字列の分析：\n\n1. 「ABCDEFGH」（8文字、全て1回）\n   ・出現頻度：全て等確率（1/8）\n   ・エントロピー：3ビット（log₂8）\n   ・固定長3ビットと同等、圧縮効果なし\n\n2. 「AAAABBCD」（8文字）\n   ・出現頻度：A=4/8, B=2/8, C=1/8, D=1/8\n   ・偏りが大きい\n   ・符号例：A=0(1ビット), B=10(2ビット), C=110(3ビット), D=111(3ビット)\n   ・合計：4×1 + 2×2 + 1×3 + 1×3 = 14ビット\n   ・固定長（3ビット×8）：24ビット\n   ・圧縮率：58%（最も効果的）\n\n3. 「AABBCCDD」（8文字）\n   ・出現頻度：全て2/8（均等）\n   ・エントロピー：2ビット\n   ・固定長2ビットと同等、圧縮効果小\n\n4. 「ABCDABCD」（8文字）\n   ・出現頻度：全て2/8（均等）\n   ・エントロピー：2ビット\n   ・繰り返しパターンだが、ハフマン符号化では考慮されない\n   ・圧縮効果小\n\n結論：「AAAABBCD」が最も圧縮効果が高い\n\n一般原則：\n・出現頻度の偏りが大きい→圧縮効果大\n・出現頻度が均等→圧縮効果小\n\n注意：\n・ハフマン符号化は繰り返しパターンを考慮しない\n・繰り返しパターンにはLZ77などの辞書式圧縮が有効"
    },
    {
      "id": "q-tech-1-3-3-010",
      "type": "multiple_choice",
      "question": "シャノンの情報理論において、情報量が0ビットになるのはどのような場合ですか？",
      "options": [
        "確率が0の事象",
        "確率が1の事象",
        "確率が0.5の事象",
        "情報量が0になることはない"
      ],
      "correctAnswer": 1,
      "explanation": "情報量が0ビットになるのは、確率が1の事象（確実に起こる事象）の場合です。\n\n情報量の計算式：\nI(x) = -log₂ P(x) [ビット]\n\n各場合の情報量：\n\n1. 確率が1の事象（確実）\n   I = -log₂(1) = 0ビット\n   例：「太陽は東から昇る」→情報量0（当たり前なので情報なし）\n\n2. 確率が0.5の事象\n   I = -log₂(0.5) = 1ビット\n   例：コインの表が出る\n\n3. 確率が0.25の事象\n   I = -log₂(0.25) = 2ビット\n\n4. 確率が0の事象\n   I = -log₂(0) = ∞（無限大）\n   絶対に起こらない事象が起きたら無限の驚き\n   ただし、実際には計算不能（0の対数は定義されない）\n\n情報量の直感的理解：\n・確率が低い事象：起こると驚き（情報量大）\n・確率が高い事象：起こっても驚かない（情報量小）\n・確実な事象：全く驚かない（情報量0）\n\n例：\n・「6面サイコロで1が出る」（確率1/6）\n  I = -log₂(1/6) ≈ 2.58ビット\n\n・「明日も地球は回る」（確率≈1）\n  I ≈ 0ビット（新しい情報はない）\n\n情報理論の応用：\n・データ圧縮：確実な情報は符号化不要\n・通信理論：不確実性の高い情報を効率的に伝送"
    }
  ]
}
